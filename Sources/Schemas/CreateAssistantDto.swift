import Foundation

public struct CreateAssistantDto: Codable, Hashable, Sendable {
    /// These are the options for the assistant's transcriber.
    public let transcriber: CreateAssistantDtoTranscriber?
    /// These are the options for the assistant's LLM.
    public let model: CreateAssistantDtoModel?
    /// These are the options for the assistant's voice.
    public let voice: CreateAssistantDtoVoice?
    /// This is the first message that the assistant will say. This can also be a URL to a containerized audio file (mp3, wav, etc.).
    /// 
    /// If unspecified, assistant will wait for user to speak and use the model to respond once they speak.
    public let firstMessage: String?
    public let firstMessageInterruptionsEnabled: Bool?
    /// This is the mode for the first message. Default is 'assistant-speaks-first'.
    /// 
    /// Use:
    /// - 'assistant-speaks-first' to have the assistant speak first.
    /// - 'assistant-waits-for-user' to have the assistant wait for the user to speak first.
    /// - 'assistant-speaks-first-with-model-generated-message' to have the assistant speak first with a message generated by the model based on the conversation state. (`assistant.model.messages` at call start, `call.messages` at squad transfer points).
    /// 
    /// @default 'assistant-speaks-first'
    public let firstMessageMode: CreateAssistantDtoFirstMessageMode?
    /// These are the settings to configure or disable voicemail detection. Alternatively, voicemail detection can be configured using the model.tools=[VoicemailTool].
    /// This uses Twilio's built-in detection while the VoicemailTool relies on the model to detect if a voicemail was reached.
    /// You can use neither of them, one of them, or both of them. By default, Twilio built-in detection is enabled while VoicemailTool is not.
    public let voicemailDetection: CreateAssistantDtoVoicemailDetection?
    /// These are the messages that will be sent to your Client SDKs. Default is conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started. You can check the shape of the messages in ClientMessage schema.
    public let clientMessages: [CreateAssistantDtoClientMessagesItem]?
    /// These are the messages that will be sent to your Server URL. Default is conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,handoff-destination-request,user-interrupted. You can check the shape of the messages in ServerMessage schema.
    public let serverMessages: [CreateAssistantDtoServerMessagesItem]?
    /// This is the maximum number of seconds that the call will last. When the call reaches this duration, it will be ended.
    /// 
    /// @default 600 (10 minutes)
    public let maxDurationSeconds: Double?
    /// This is the background sound in the call. Default for phone calls is 'office' and default for web calls is 'off'.
    /// You can also provide a custom sound by providing a URL to an audio file.
    public let backgroundSound: CreateAssistantDtoBackgroundSound?
    /// This determines whether the model's output is used in conversation history rather than the transcription of assistant's speech.
    /// 
    /// Default `false` while in beta.
    /// 
    /// @default false
    public let modelOutputInMessagesEnabled: Bool?
    /// These are the configurations to be passed to the transport providers of assistant's calls, like Twilio. You can store multiple configurations for different transport providers. For a call, only the configuration matching the call transport provider is used.
    public let transportConfigurations: [TransportConfigurationTwilio]?
    /// This is the plan for observability of assistant's calls.
    /// 
    /// Currently, only Langfuse is supported.
    public let observabilityPlan: LangfuseObservabilityPlan?
    /// These are dynamic credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can supplement an additional credentials using this. Dynamic credentials override existing credentials.
    public let credentials: [CreateAssistantDtoCredentialsItem]?
    /// This is a set of actions that will be performed on certain events.
    public let hooks: [CreateAssistantDtoHooksItem]?
    /// This is the name of the assistant.
    /// 
    /// This is required when you want to transfer between assistants in a call.
    public let name: String?
    /// This is the message that the assistant will say if the call is forwarded to voicemail.
    /// 
    /// If unspecified, it will hang up.
    public let voicemailMessage: String?
    /// This is the message that the assistant will say if it ends the call.
    /// 
    /// If unspecified, it will hang up without saying anything.
    public let endCallMessage: String?
    /// This list contains phrases that, if spoken by the assistant, will trigger the call to be hung up. Case insensitive.
    public let endCallPhrases: [String]?
    public let compliancePlan: CompliancePlan?
    /// This is for metadata you want to store on the assistant.
    public let metadata: [String: JSONValue]?
    /// This enables filtering of noise and background speech while the user is talking.
    /// 
    /// Features:
    /// - Smart denoising using Krisp
    /// - Fourier denoising
    /// 
    /// Smart denoising can be combined with or used independently of Fourier denoising.
    /// 
    /// Order of precedence:
    /// - Smart denoising
    /// - Fourier denoising
    public let backgroundSpeechDenoisingPlan: BackgroundSpeechDenoisingPlan?
    /// This is the plan for analysis of assistant's calls. Stored in `call.analysis`.
    public let analysisPlan: AnalysisPlan?
    /// This is the plan for artifacts generated during assistant's calls. Stored in `call.artifact`.
    public let artifactPlan: ArtifactPlan?
    /// This is the plan for when the assistant should start talking.
    /// 
    /// You should configure this if you're running into these issues:
    /// - The assistant is too slow to start talking after the customer is done speaking.
    /// - The assistant is too fast to start talking after the customer is done speaking.
    /// - The assistant is so fast that it's actually interrupting the customer.
    public let startSpeakingPlan: StartSpeakingPlan?
    /// This is the plan for when assistant should stop talking on customer interruption.
    /// 
    /// You should configure this if you're running into these issues:
    /// - The assistant is too slow to recognize customer's interruption.
    /// - The assistant is too fast to recognize customer's interruption.
    /// - The assistant is getting interrupted by phrases that are just acknowledgments.
    /// - The assistant is getting interrupted by background noises.
    /// - The assistant is not properly stopping -- it starts talking right after getting interrupted.
    public let stopSpeakingPlan: StopSpeakingPlan?
    /// This is the plan for real-time monitoring of the assistant's calls.
    /// 
    /// Usage:
    /// - To enable live listening of the assistant's calls, set `monitorPlan.listenEnabled` to `true`.
    /// - To enable live control of the assistant's calls, set `monitorPlan.controlEnabled` to `true`.
    public let monitorPlan: MonitorPlan?
    /// These are the credentials that will be used for the assistant calls. By default, all the credentials are available for use in the call but you can provide a subset using this.
    public let credentialIds: [String]?
    /// This is where Vapi will send webhooks. You can find all webhooks available along with their shape in ServerMessage schema.
    /// 
    /// The order of precedence is:
    /// 
    /// 1. assistant.server.url
    /// 2. phoneNumber.serverUrl
    /// 3. org.serverUrl
    public let server: Server?
    public let keypadInputPlan: KeypadInputPlan?
    /// Additional properties that are not explicitly defined in the schema
    public let additionalProperties: [String: JSONValue]

    public init(
        transcriber: CreateAssistantDtoTranscriber? = nil,
        model: CreateAssistantDtoModel? = nil,
        voice: CreateAssistantDtoVoice? = nil,
        firstMessage: String? = nil,
        firstMessageInterruptionsEnabled: Bool? = nil,
        firstMessageMode: CreateAssistantDtoFirstMessageMode? = nil,
        voicemailDetection: CreateAssistantDtoVoicemailDetection? = nil,
        clientMessages: [CreateAssistantDtoClientMessagesItem]? = nil,
        serverMessages: [CreateAssistantDtoServerMessagesItem]? = nil,
        maxDurationSeconds: Double? = nil,
        backgroundSound: CreateAssistantDtoBackgroundSound? = nil,
        modelOutputInMessagesEnabled: Bool? = nil,
        transportConfigurations: [TransportConfigurationTwilio]? = nil,
        observabilityPlan: LangfuseObservabilityPlan? = nil,
        credentials: [CreateAssistantDtoCredentialsItem]? = nil,
        hooks: [CreateAssistantDtoHooksItem]? = nil,
        name: String? = nil,
        voicemailMessage: String? = nil,
        endCallMessage: String? = nil,
        endCallPhrases: [String]? = nil,
        compliancePlan: CompliancePlan? = nil,
        metadata: [String: JSONValue]? = nil,
        backgroundSpeechDenoisingPlan: BackgroundSpeechDenoisingPlan? = nil,
        analysisPlan: AnalysisPlan? = nil,
        artifactPlan: ArtifactPlan? = nil,
        startSpeakingPlan: StartSpeakingPlan? = nil,
        stopSpeakingPlan: StopSpeakingPlan? = nil,
        monitorPlan: MonitorPlan? = nil,
        credentialIds: [String]? = nil,
        server: Server? = nil,
        keypadInputPlan: KeypadInputPlan? = nil,
        additionalProperties: [String: JSONValue] = .init()
    ) {
        self.transcriber = transcriber
        self.model = model
        self.voice = voice
        self.firstMessage = firstMessage
        self.firstMessageInterruptionsEnabled = firstMessageInterruptionsEnabled
        self.firstMessageMode = firstMessageMode
        self.voicemailDetection = voicemailDetection
        self.clientMessages = clientMessages
        self.serverMessages = serverMessages
        self.maxDurationSeconds = maxDurationSeconds
        self.backgroundSound = backgroundSound
        self.modelOutputInMessagesEnabled = modelOutputInMessagesEnabled
        self.transportConfigurations = transportConfigurations
        self.observabilityPlan = observabilityPlan
        self.credentials = credentials
        self.hooks = hooks
        self.name = name
        self.voicemailMessage = voicemailMessage
        self.endCallMessage = endCallMessage
        self.endCallPhrases = endCallPhrases
        self.compliancePlan = compliancePlan
        self.metadata = metadata
        self.backgroundSpeechDenoisingPlan = backgroundSpeechDenoisingPlan
        self.analysisPlan = analysisPlan
        self.artifactPlan = artifactPlan
        self.startSpeakingPlan = startSpeakingPlan
        self.stopSpeakingPlan = stopSpeakingPlan
        self.monitorPlan = monitorPlan
        self.credentialIds = credentialIds
        self.server = server
        self.keypadInputPlan = keypadInputPlan
        self.additionalProperties = additionalProperties
    }

    public init(from decoder: Decoder) throws {
        let container = try decoder.container(keyedBy: CodingKeys.self)
        self.transcriber = try container.decodeIfPresent(CreateAssistantDtoTranscriber.self, forKey: .transcriber)
        self.model = try container.decodeIfPresent(CreateAssistantDtoModel.self, forKey: .model)
        self.voice = try container.decodeIfPresent(CreateAssistantDtoVoice.self, forKey: .voice)
        self.firstMessage = try container.decodeIfPresent(String.self, forKey: .firstMessage)
        self.firstMessageInterruptionsEnabled = try container.decodeIfPresent(Bool.self, forKey: .firstMessageInterruptionsEnabled)
        self.firstMessageMode = try container.decodeIfPresent(CreateAssistantDtoFirstMessageMode.self, forKey: .firstMessageMode)
        self.voicemailDetection = try container.decodeIfPresent(CreateAssistantDtoVoicemailDetection.self, forKey: .voicemailDetection)
        self.clientMessages = try container.decodeIfPresent([CreateAssistantDtoClientMessagesItem].self, forKey: .clientMessages)
        self.serverMessages = try container.decodeIfPresent([CreateAssistantDtoServerMessagesItem].self, forKey: .serverMessages)
        self.maxDurationSeconds = try container.decodeIfPresent(Double.self, forKey: .maxDurationSeconds)
        self.backgroundSound = try container.decodeIfPresent(CreateAssistantDtoBackgroundSound.self, forKey: .backgroundSound)
        self.modelOutputInMessagesEnabled = try container.decodeIfPresent(Bool.self, forKey: .modelOutputInMessagesEnabled)
        self.transportConfigurations = try container.decodeIfPresent([TransportConfigurationTwilio].self, forKey: .transportConfigurations)
        self.observabilityPlan = try container.decodeIfPresent(LangfuseObservabilityPlan.self, forKey: .observabilityPlan)
        self.credentials = try container.decodeIfPresent([CreateAssistantDtoCredentialsItem].self, forKey: .credentials)
        self.hooks = try container.decodeIfPresent([CreateAssistantDtoHooksItem].self, forKey: .hooks)
        self.name = try container.decodeIfPresent(String.self, forKey: .name)
        self.voicemailMessage = try container.decodeIfPresent(String.self, forKey: .voicemailMessage)
        self.endCallMessage = try container.decodeIfPresent(String.self, forKey: .endCallMessage)
        self.endCallPhrases = try container.decodeIfPresent([String].self, forKey: .endCallPhrases)
        self.compliancePlan = try container.decodeIfPresent(CompliancePlan.self, forKey: .compliancePlan)
        self.metadata = try container.decodeIfPresent([String: JSONValue].self, forKey: .metadata)
        self.backgroundSpeechDenoisingPlan = try container.decodeIfPresent(BackgroundSpeechDenoisingPlan.self, forKey: .backgroundSpeechDenoisingPlan)
        self.analysisPlan = try container.decodeIfPresent(AnalysisPlan.self, forKey: .analysisPlan)
        self.artifactPlan = try container.decodeIfPresent(ArtifactPlan.self, forKey: .artifactPlan)
        self.startSpeakingPlan = try container.decodeIfPresent(StartSpeakingPlan.self, forKey: .startSpeakingPlan)
        self.stopSpeakingPlan = try container.decodeIfPresent(StopSpeakingPlan.self, forKey: .stopSpeakingPlan)
        self.monitorPlan = try container.decodeIfPresent(MonitorPlan.self, forKey: .monitorPlan)
        self.credentialIds = try container.decodeIfPresent([String].self, forKey: .credentialIds)
        self.server = try container.decodeIfPresent(Server.self, forKey: .server)
        self.keypadInputPlan = try container.decodeIfPresent(KeypadInputPlan.self, forKey: .keypadInputPlan)
        self.additionalProperties = try decoder.decodeAdditionalProperties(using: CodingKeys.self)
    }

    public func encode(to encoder: Encoder) throws -> Void {
        var container = encoder.container(keyedBy: CodingKeys.self)
        try encoder.encodeAdditionalProperties(self.additionalProperties)
        try container.encodeIfPresent(self.transcriber, forKey: .transcriber)
        try container.encodeIfPresent(self.model, forKey: .model)
        try container.encodeIfPresent(self.voice, forKey: .voice)
        try container.encodeIfPresent(self.firstMessage, forKey: .firstMessage)
        try container.encodeIfPresent(self.firstMessageInterruptionsEnabled, forKey: .firstMessageInterruptionsEnabled)
        try container.encodeIfPresent(self.firstMessageMode, forKey: .firstMessageMode)
        try container.encodeIfPresent(self.voicemailDetection, forKey: .voicemailDetection)
        try container.encodeIfPresent(self.clientMessages, forKey: .clientMessages)
        try container.encodeIfPresent(self.serverMessages, forKey: .serverMessages)
        try container.encodeIfPresent(self.maxDurationSeconds, forKey: .maxDurationSeconds)
        try container.encodeIfPresent(self.backgroundSound, forKey: .backgroundSound)
        try container.encodeIfPresent(self.modelOutputInMessagesEnabled, forKey: .modelOutputInMessagesEnabled)
        try container.encodeIfPresent(self.transportConfigurations, forKey: .transportConfigurations)
        try container.encodeIfPresent(self.observabilityPlan, forKey: .observabilityPlan)
        try container.encodeIfPresent(self.credentials, forKey: .credentials)
        try container.encodeIfPresent(self.hooks, forKey: .hooks)
        try container.encodeIfPresent(self.name, forKey: .name)
        try container.encodeIfPresent(self.voicemailMessage, forKey: .voicemailMessage)
        try container.encodeIfPresent(self.endCallMessage, forKey: .endCallMessage)
        try container.encodeIfPresent(self.endCallPhrases, forKey: .endCallPhrases)
        try container.encodeIfPresent(self.compliancePlan, forKey: .compliancePlan)
        try container.encodeIfPresent(self.metadata, forKey: .metadata)
        try container.encodeIfPresent(self.backgroundSpeechDenoisingPlan, forKey: .backgroundSpeechDenoisingPlan)
        try container.encodeIfPresent(self.analysisPlan, forKey: .analysisPlan)
        try container.encodeIfPresent(self.artifactPlan, forKey: .artifactPlan)
        try container.encodeIfPresent(self.startSpeakingPlan, forKey: .startSpeakingPlan)
        try container.encodeIfPresent(self.stopSpeakingPlan, forKey: .stopSpeakingPlan)
        try container.encodeIfPresent(self.monitorPlan, forKey: .monitorPlan)
        try container.encodeIfPresent(self.credentialIds, forKey: .credentialIds)
        try container.encodeIfPresent(self.server, forKey: .server)
        try container.encodeIfPresent(self.keypadInputPlan, forKey: .keypadInputPlan)
    }

    /// Keys for encoding/decoding struct properties.
    enum CodingKeys: String, CodingKey, CaseIterable {
        case transcriber
        case model
        case voice
        case firstMessage
        case firstMessageInterruptionsEnabled
        case firstMessageMode
        case voicemailDetection
        case clientMessages
        case serverMessages
        case maxDurationSeconds
        case backgroundSound
        case modelOutputInMessagesEnabled
        case transportConfigurations
        case observabilityPlan
        case credentials
        case hooks
        case name
        case voicemailMessage
        case endCallMessage
        case endCallPhrases
        case compliancePlan
        case metadata
        case backgroundSpeechDenoisingPlan
        case analysisPlan
        case artifactPlan
        case startSpeakingPlan
        case stopSpeakingPlan
        case monitorPlan
        case credentialIds
        case server
        case keypadInputPlan
    }
}